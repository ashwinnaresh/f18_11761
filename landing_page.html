<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <title>11-661/11-761 Language and Statistics</title>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-MfvZlkHCEqatNoGiOXveE8FIwMzZg4W85qfrfIFBfYc= sha512-dTfge/zgoMYpP7QbHy4gWMEGsbsdZeCXz7irItjcC3sPUFtf0kuFbDz/ixG7ArTxmDjLXDmezHubeNikyKGVyQ==" crossorigin="anonymous">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,400italic,300italic' rel='stylesheet' type='text/css'>
    <link rel="shortcut icon" href="http://www.qatar.cmu.edu/cs/10601/img/favicon.ico">
    <style>
        body {
            font-family: 'Open Sans', sans-serif;
            font-size: 1.5em;
            font-weight: 300;
            text-align: justify;
        }

        b {
            font-weight: 400;
        }

        footer {
            margin-top: 60px;
            margin-bottom: 30px;
            text-align: center;
        }

        h2 {
            color: #b05145;
        }

        h3 {
            color: #bf766d;
            font-weight: 300;
        }

        a, a:hover {
            color: #b05145;
        }

        .container {
            max-width: 1200px;
            padding-left: 25px;
            padding-right: 25px;
        }

        .titlebar {
            padding-top: 40px;
            padding-bottom: 40px;
            overflow: auto;
            white-space: normal;
        }

        @media (max-width: 1200px) {
            .titlebar {
                text-align: center;
            }
        }

        .titlebar-img {
            width: 180px;
            height: 180px;
            /*float: left;*/
        }

        .titlebar-text {
            max-width: 66.66666667%;
            float: left;
            white-space: normal;
        }

        .title-col {
            text-align: center;
            display: inline;
        }

        .title {
            font-size: 3em;
        }

        .subtitle {
            font-size: 2em;
        }

        .infoline {
            width: 100%;
        }

        .infoline-heading {
            width: 130px;
            text-align: right;
            display: inline-block;
        }

        .infoline-text {
            width: 50%;
            text-align: left;
            display: inline-block;
            vertical-align: text-top;
        }

        .grade-table {
            width: 100%;
        }

        .grade-table-header {
            border-bottom-width: 1px;
            border-bottom-style: solid;
        }

        .grade-table-bottom {
            border-top-width: 1px;
            border-top-style: solid;
        }

        .textbook {
            display: block;
            height: 95px;
            margin-bottom: 10px;
        }

        .textbook-img-container {
            display: inline-block;
            float: left;
            margin-right: 10px;
        }

            .textbook-img-container img {
                height: 95px;
                width: 70px;
                border-radius: 2px;
            }

        .textbook-info-item {
            display: block;
        }

        .vcenter {
            display: inline-block;
            vertical-align: middle;
            float: none;
        }

        #logo {
            width: 90%;
            height: auto;
        }

        .ul {
            list-style: none;
            padding-left: 1.2em;
        }


    </style>
</head>

<body>
    <div class="container titlebar">
        <div class="row">
            <!--div class="titlebar-img title-col vcenter">
                <img id="logo" src="./img/brain.png">
            </div-->
            <div class="title-col vcenter" style="color:#A80000;font-weight:bold;text-align:center;width:100%">
                <div class="title"><b>11-661/761</b> Language and Statistics</div>
                <div class="subtitle"><i>Fall 2018</i></div>
            </div>
        </div>
    </div>

    <div class=" container">
        <div class="row">
            <p style="margin-top:75px;">
                Internet search, speech recognition, machine translation, question answering, information retrieval, biological sequence analysis -- are all at the forefront of this century’s information revolution. In addition to their use of machine learning, these technologies rely heavily on classic statistical estimation techniques. Yet most CS and engineering undergraduate programs do not prepare students in this area beyond an introductory probability & statistics course. This course is designed to address this gap.
            </p>
            <p> The goal of "Language and Statistics" is to ground the data-driven techniques used in language technologies in sound statistical methodology. We start by formulating various language technology problems in both an information theoretic framework (the source-channel paradigm) and a Bayesian framework (the Bayes classifier). We then discuss the statistical properties of words, sentences, documents and whole languages, and the various computational formalisms used to represent language. These discussions naturally lead to specific concepts in statistical estimation.</p>
            
            <p>Topics include: Zipf's distribution and type-token curves; point estimators, Maximum Likelihood estimation, bias and variance, sparseness, smoothing and clustering; interpolation, shrinkage, and backoff; entropy, cross entropy and mutual information; decision tree models applied to language; latent variable models and the EM algorithm; hidden Markov models; exponential models and the maximum entropy principle; semantic modeling and dimensionality reduction; probabilistic context-free grammars and syntactic language models.</p>

            <p>“Language and Statistics” is designed for LTI and other SCS graduate students, but others are also welcome. CS undergraduate upperclassmen who have taken it have done well, although they found it challenging.</p>

            <p>The Master-level version of this course (11-661) does not require the course project.</p>

            <p><b style="color:#A80000;">Instructor:</b> Bhiksha Raj</p>
            <ul style="margin-top:-10px;">
                <li>bhiksha@cs.cmu.edu</li>
            </ul>
            <p>
                <b style="color:#A80000;">TAs:</b>
                <ul>
                    <li> Pramati Kalwad (pkalwad@andrew.cmu.edu) </li>
                    <li> Ashwin Naresh Kumar (anareshk@andrew.cmu.edu) </li>
                    <li> Zhilin Yang (zhiliny@andrew.cmu.edu) </li>
                </ul>
                <!-- <p style="font-size: 12px;">* contingent on registration</p> -->
            <p><b style="color:#A80000;">Lecture:</b> Monday and Wednesday, 12:00 noon - 1.20pm</p>
            <p><b style="color:#A80000;">Location:</b> Gates-Hillman Complex GHC 4307</p>
            <p><b style="color:#A80000;">Recitation:</b> TBD</p>
            <p>
                <b style="color:#A80000;">Office hours:</b>
                <ul>
                    <!-- <li> Tuesday @ 3:30pm-5:00pm Location TBD</li>
                    <li> Friday @ 10:30am-12:00pm NSH Atrium</li>
                    <li> Doha: Sunday and Tuesday @ 5:00pm-6:30pm (Quatar Time)</li>
                    <li> Kigali: TBD</li>
                    <li> Bhiksha: Thursday, 11:00am-12:00pm</li> -->
                    <li> TBD </li>
                </ul>
            </p>
            <h3>Prerequisites</h3>
                <p>Strong quantitative aptitude. Familiarity and comfort with basic undergraduate-level probability. Some programming skill.</p>
            <h3>Units</h3>
            <p>This course is worth 12 units.</p>
        </div>
    </div>
    <div class="container">
        <div class="row">
            <h2>Course Work</h2>
            <div>
                <table class="grade-table">
                    <tr class="grade-table-header"> <td></td> <td><b>Maximum</b></td> </tr>
                    <tr> <td><b>Exam</b></td> <td>1 exam, total contribution to grade 35%</td></tr>
                    <tr> <td><b>Assignments</b></td> <td>7 assignments, total contribution to grade 35%</td></tr>
                    <tr> <td><b>Final Project</b></td> <td>1 project, total contribution to grade 35% (only for those taking 11-761; otherwise, renormalize)</td></tr>
                </table>
            </div>
            <ul>
                <li>Top “Endorsed Answer” Answerers on Piazza may get bonus points.</li>
                <li>Class participation can also improve your grade.</li>
            </ul>
            <h4>Late Policy</h4>
            <p> <b>Extensions: </b>If you have an unavoidable conflict that prevents you from completing an assignment on time (such as travel to a conference or a medical emergency), please send an email to the TA in charge of that assignment, as soon as you become aware of the problem, briefly stating the circumstances and how much more time you need.  The TA-in-charge is authorized to grant an extension as long as you requested it promptly.  Do not send extension requests to the Instructor, nor to the other TAs.</p>
            <p>Assignments turned late without prior approval will incur a 25% penalty per 24-hour period or any part thereof.</p>


            <h3>Books</h3>
            <p>The course will not follow a specific book, but will draw from a number of sources. We list relevant books at the end of this page. We will also put up links to relevant reading material for each class. Students are expected to familiarize themselves with the material before the class. The readings will sometimes be arcane and difficult to understand; if so, do not worry, we will present simpler explanations in class.</p>
            <h3>Discussion board: Piazza</h3>
            
            <p>We will use Piazza for discussions. <a href="#">Here is the link</a>. Please sign up.</p>
            <!-- <h3>Wiki page</h3>
            <p>We have created an experimental wiki explaining the types of neural networks in use today. <a href="https://www.contrib.andrew.cmu.edu/~dalud/deep-learning-wiki/doku.php">Here is the link</a>.</p> -->
            
            <h3>Academic Integrity</h3>
            <div>
                You are expected to comply with the <a href="http://www.cmu.edu/policies/documents/Cheating.html">University Policy on Academic Integrity and Plagiarism</a>.
                <ul>
                    <li>You are allowed to talk with / work with other students on homework assignments</li>
                    <li>You can share ideas but not code, you should submit your own code</li>
                </ul>
                Your course instructor reserves the right to determine an appropriate penalty based on the violation of academic dishonesty that occurs. Violations of the university policy can result in severe penalties including failing this course and possible expulsion from Carnegie Mellon University. If you have any questions about this policy and any work you are doing in the course, please feel free to contact your instructor for help.
            </div>
        </div>
    </div>
    <div class="container">
        <div class="row">
            <h2>Tentative Schedule</h2>
            <div class="">
                <table class="table table-striped table-bordered">
                    <thead>
                        <tr>
                            <th>Lectures</th>
                            <th>Dates</th>
                            <th>Topics</th>
                            <th>Required Reading (due BEFORE class)</th>
                            <th>Lecture notes/Slides</th>
                            <th>Additional readings, if any</th>
                            <th>Assignments</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1,2</td>
                            <td>August 29, September 3</td>
                            <td>
                                <ul>
                                    <li>Course Overview</li>
                                    <li>Statistical Language Modeling</li>
                                    <li>Computational Linguistics</li>
                                    <li>Statistical Decision Making</li>
                                    <li>Source-Channel Paradigm</li>
                                </ul>
                            </td>
                            <td>[MS] 1.1 - 1.3, 2.1</td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td><a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/intro.pdf">Historical overview by Lafferty</a></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>September 5</td>
                            <td>
                                <ul>
                                    <li>All About Words: Types, Tokens and Vocabularies</li> 
                                    <li>Type-Token Data</li>
                                </ul>
                            </td>
                            <td>[MS] 1.4</td>
			                 <td>
                    <!-- LN -->
                        </td>
                            <td>
                                [BCW] ch. 4, <a href="https://www.youtube.com/watch?v=fCn8zs912OE">Zipf's Law</a>
                            </td>
                            <td> </td>
                        </tr>
                        <tr>
                            <td>4,5</td>
                            <td>September 10, Spetember 12</td>
                            <td>
                                <ul>
                                    <li>Unigrams: Statistical Estimation</li>
                                    <li>Maximum Likelihood Estimates</li>
                                </ul>
                            </td>
                            <td>[MS] 6.2.1</td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                               [mD] ch. 6, esp. 6.5
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>6,7</td>
                            <td>September 17, September 19</td>
                            <td>
                                <ul>
                                    <li>Sparseness</li>
                                    <li>Smoothing</li>
                                </ul>
                            </td>
                            <td>[MS] 6.2.2</td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                                <a href="http://www.ling.upenn.edu/courses/cogs502/GoodTuring1953.pdf">Good-Turing</a>
                            </td>
                            
                            <td></td>
                        </tr>
                        <tr>
                            <td>8,9</td>
                            <td>September 24, September 26</td>
                            <td>
                                <ul>
                                    <li>N-grams: Linear Interpolation</li>
                                    <li>Backoff</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li>[MS] 6.1, 6.3 </li>
                                    <li><a href="http://ieeexplore.ieee.org/iel6/29/26208/01165125.pdf?isnumber=&arnumber=1165125">[sK]</a></li>
                                </ul>
                            </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/Presentations/h015a-techreport.pdf">Chen & Goodman 98</a>(pp. 1-21)
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>10,11,12,13</td>
                            <td>September 26, Oct 1, Oct 3, Oct 8</td>
                            <td>
                                <ul>Measuring Success:
                                    <li>Information Theory</li> 
                                    <li>Entropy</li> 
                                    <li>Perplexity</li> 
                                </ul>
                            </td>
                            <td>
			                     [MS] 2.2
                            </td>
                            <td>
                            <!-- LN -->
                            </td>
                            <td>
                                <ul>
                                    <li><a href="https://web.cse.msu.edu/~cse842/Papers/CoverThomas-Ch2.pdf">Elements of Information Theory</a></li>
                                    <li><a href="http://colah.github.io/posts/2015-09-Visual-Information/">Visual Information Theory</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/gauntlet.pdf">IT notes: Entropy of English</a></li>
                                </ul>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>14</td>
                            <td>Oct 10</td>
                            <td>Clustering</td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/classlm.pdf">Class LM</a>
                            </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                                <ul>
                                    <li>[MS] 14.1</li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/papers/lattice-TR-97-173.pdf">Lattice LM</a></li>
                                </ul>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>15,16,17</td>
                            <td>Oct 15, Oct 17, Oct 22</td>
                            <td>
                                <ul>
                                    <li>Hidden Markov Models</li>
                                </ul>
                            </td>
                            <td>
                               [MS] ch. 9
			                 </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/Presentations/rabiner.pdf">Larry Rabiner's classic HMM tutorial</a>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>18,19,20,21</td>
                            <td>Oct 29, Oct 31, Nov 5, Nov 7</td>
                            <td>
                                <ul>
                                    <li>Maximum Entropy Models</li>
                                    <li>Whole-Sentence Models</li>
                                    <li>Semantic Modeling</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li><a href="http://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">Adam Berger's online tutorial</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/Presentations/convexity-maximum-likelihood-and.pdf">Convexity, Maximum Likelihood, and All That</a></li>
                                </ul>
                            </td>
                            <td>
                               <!-- LN -->
			                 </td>
                            <td>
                                <ul> 
                                    <li>[MS] 16.2</li>
                                    <li><a href="http://www.cs.cmu.edu/~nasmith/papers/smith.tut04.pdf">Noah Smith's tutorial</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/ME-compling.pdf">[BDD]</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/papers/me-csl-revised.pdf">[rR]</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/Presentations/MaxEnt-Rosenfeld94.pdf">[rR slides]</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/Presentations/Modeling-MCMC-Logistic-Regression.pdf">using MCMC with language</a></li>
                                </ul>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>22</td>
                            <td>Nov 12</td>
                            <td>
                                <ul>
                                    <li>Latent Variable Models</li>
                                    <li>EM Algorithm</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li>[MS] 14.2</li>
                                    <li>Notes by Guy Lebanon: <a href="http://www.cs.cmu.edu/~roni/11761/Presentations/emExample.pdf">Derivation of EM for Gaussian mixture</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/Presentations/emShortcut.pdf">EM derivation shortcut for exponential family</a></li>
                                </ul>
                            </td>
                            <td>
                                <!--LN -->
                            </td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/em.pdf">More advanced EM notes by John Lafferty</a>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>23</td>
                            <td></td>
                            <td>Review</td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>24</td>
                            <td></td>
                            <td>Exam</td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>25</td>
                            <td>November 19</td>
                            <td>
                                <ul>
                                    <li>Probabilistic Context Free Grammars (PCFG)</li>
                                    <li>The Inside-Outside Algorithm</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/pcfg-notes.pdf">Notes on Probabilistic Context Free Grammars</a></li>
                                </ul>
                            </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>[MS] 11.1-11.4</td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>26</td>
                            <td>November 21</td>
                            <td>
                                <ul>Semantic representation (embedding):
                                    <li>Latent Semantic Analysis</li>
                                    <li>Dimensionality Reduction</li>
                                    <li>cf. Word2vec</li>
                                </ul>
                            </td>
                            <td>
                                <ul>
                                <li><a href="http://ieeexplore.ieee.org/iel4/89/15387/00709671.pdf">Bellegarda 99</a></li>
                                    <li><a href="http://www.cob.unt.edu/itds/faculty/evangelopoulos/dsci5910/LSA_Deerwester1990.pdf">Indexing by latent semantic analysis</a></li>
                                </ul>
                            </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>
                                <ul>
                                    <li><a href="http://ieeexplore.ieee.org/iel5/89/17730/00817455.pdf">Bellegarda 00</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~roni/11761/Presentations/lsi.pdf">Yan Liu's Slides</a></li>
                                    <li><a href="http://www.cs.brown.edu/~th/papers/Hofmann-UAI99.pdf">Hoffmann 99</a></li>
                                    <li><a href="http://www.cs.brown.edu/people/th/papers/GildeaHofmann-EUROSPEECH99.pdf">Gildea and Hofmann 99</a></li>
                                    <li><a href="http://www.cs.cmu.edu/~antoine/papers/icslp2004a.pdf">Raux and Singh 04</a></li>
                                </ul>
                            </td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>27,28</td>
                            <td></td>
                            <td>Exam</td>
                            <td><b>Mandatory Class Attendance</b></td>
                            <td></td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Overflow Material</td>
                        </tr>
                        <tr>
                            <td>29</td>
                            <td>November 26</td>
                            <td>
                                <ul>
                                    <li>Syntactic Language Models</li>
                                </ul>
                            </td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/Presentations/camera_ready.EUROSP99.fred.pdf">Jelinek and Chelba 99</a>
                            </td>
                            <td></td>
                            <td>
                                <a href="http://www.cs.cmu.edu/~roni/11761/Presentations/slides.ACL98.pdf">Chelba Slides 98</a>
                            </td>
                            <td>
                            </td>
                        </tr>
                        <tr>
                            <td>30</td>
                            <td>November 28</td>
                            <td>
                                <ul>
                                    <li>Decision Tree Language Models</li>
                                </ul>
                            </td>
                            <td>
                                <a href="http://ieeexplore.ieee.org/iel1/29/1399/00032278.pdf">[BBDM]</a>
                            </td>
                            <td>
                                <!-- LN -->
                            </td>
                            <td>[MS] 16.1</td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            <h3> Abbreviations (in order of appearance): </h3>
            <table class="table table-striped table-bordered">
                <thead>
                    <tr>
                        <td>Abbreviation</td>
                        <td>Book</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>[MS]</td>
                        <td>Manning and Schutze, Foundations of Statistical Natural Language Processing.</td>
                    </tr>
                    <tr>
                        <td>[BCW]</td>
                        <td>Bell, Cleary and Witten, Text Compression.</td>
                    </tr>
                    <tr>
                        <td>[mD]</td>
                        <td>Morris DeGroot, Probability and Statistics, 2nd edition.</td>
                    </tr>
                    <tr>
                        <td>[sK]</td>
                        <td>Slava M. Katz, "Estimation of probabilities from sparse data for the language model component of a speech recognizer", IEEE Transactions on Acoustic, Speech and Signal Processing, vol. 35, no. 3, pp. 400-401, 1987.</td>
                    </tr>
                    <tr>
                        <td>[BBDM]</td>
                        <td>L. Bahl, P. Brown, P. de Souza and R. Mercer, "A tree-based statistical language model for natural language speech recognition", IEEE Transactions on Acoustic, Speech and Signal Processing, vol. 37, no. 7, pp. 1001-1008, 1989.</td>
                    </tr>
                    <tr>
                        <td>[rR]</td>
                        <td>Roni Rosenfeld, "A maximum entropy approach to adaptive statistical language modeling", Speech and Language, vol. 10, pp. 187-228, 1996.</td>
                    </tr>
                </tbody>
            </table>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="row">
        <!-- <h3><a href="slides/ProjectIdeas.pptx">Some ideas for projects</a></h3> -->
        </div>
    </div>
    <div class="container">
        <div class="row">
            <h2>Documentation and Tools</h2>
            <h3>Textbooks</h3>
            <div class="textbook">
                <div class="textbook-img-container"><img src="./img/Goodfellow.jpg" alt="Deep Learning" /></div>
                <div class="textbook-info">
                    <span class="textbook-info-item"><a href="http://deeplearning.cs.cmu.edu/data/DeepLearningBook.zip"><b>Deep Learning</b></a></span>
                    <span class="textbook-info-item">By Ian Goodfellow, Yoshua Bengio, Aaron Courville</span>
                    <span class="textbook-info-item"><i>Online book, 2017</i></span>
                </div>
            </div>
            <div class="textbook">
                <div class="textbook-img-container"><img src="./img/Nielsen.png" alt="Neural Networks and Deep Learning" /></div>
                <div class="textbook-info">
                    <span class="textbook-info-item"><a href="http://neuralnetworksanddeeplearning.com/"><b>Neural Networks and Deep Learning</b></a></span>
                    <span class="textbook-info-item">By Michael Nielsen</span>
                    <span class="textbook-info-item"><i>Online book, 2016</i></span>
                </div>
            </div>
            <div class="textbook">
                <div class="textbook-img-container"><img src="./img/Brownlee.png" alt="Deep Learning with Python"/></div>
                <div class="textbook-info">
                    <span class="textbook-info-item"><a href="https://machinelearningmastery.com/deep-learning-with-python/"><b>Deep Learning with Python</b></a></span>
                    <span class="textbook-info-item">By J. Brownlee</span>
                    <span class="textbook-info-item"><i></i></span>
                </div>
            </div>
            <div class="textbook">
                <div class="textbook-img-container"><img src="./img/Lewis.jpg" alt="Deep Learning Step by Step with Python"/></div>
                <div class="textbook-info">
                    <span class="textbook-info-item"><a href="https://www.amazon.com/Deep-Learning-Step-Python-Introduction/dp/1535410264"><b>Deep Learning Step by Step with Python: A Very Gentle Introduction to Deep Neural Networks for Practical Data Science</b></a></span>
                    <span class="textbook-info-item">By N. D. Lewis</span>
                    <span class="textbook-info-item"><i></i></span>
                </div>
            </div>
            <div class="textbook">
                <div class="textbook-img-container"><img src="./img/Rumelhart.jpg" alt="Parallel Distributed Processing"/></div>
                <div class="textbook-info">
                    <span class="textbook-info-item"><a href="https://mitpress.mit.edu/books/parallel-distributed-processing"><b>Parallel Distributed Processing</b></a></span>
                    <span class="textbook-info-item">By Rumelhart and McClelland</span>
                    <span class="textbook-info-item"><i>Out of print, 1986</i></span>
                </div>
            </div>
        </div>
    </div>
    <div class=" container">
        <div class="row">
</div>
</div>
</body>
</html>
